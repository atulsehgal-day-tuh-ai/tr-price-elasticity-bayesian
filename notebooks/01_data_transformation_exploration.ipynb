{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Transformation Exploration (raw CSV → model-ready)\n",
        "\n",
        "This notebook helps you validate the **data transformation pipeline** step-by-step before fitting Bayesian models.\n",
        "\n",
        "## What you’ll get confidence in\n",
        "\n",
        "- Input files load correctly\n",
        "- Retailer-specific parsing rules are applied correctly (Circana vs Costco CRX)\n",
        "- Expected columns exist after transformation\n",
        "- Prices / sales / logs look sensible (no weird zeros, negative values, or extreme outliers)\n",
        "- Retailer separation behaves as expected (`retailer_filter`)\n",
        "- “Missing feature” masking works (e.g., Costco missing competitor / private label)\n",
        "- You can export an auditable `prepared_data_from_notebook.csv`\n",
        "\n",
        "## Prereqs\n",
        "\n",
        "- Place your raw files in `data/`:\n",
        "  - `data/bjs.csv`\n",
        "  - `data/sams.csv`\n",
        "  - `data/costco.csv` (optional)\n",
        "\n",
        "- Install dependencies:\n",
        "\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "## Costco note\n",
        "\n",
        "Costco CRX typically has a different schema than Circana. The pipeline handles this via **runtime YAML retailer contracts** (`data.retailer_data_contracts` in `config_template.yaml`). This notebook will attempt to load those contracts so Costco is parsed correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook environment setup (imports + paths + outputs)\n",
        "\n",
        "**What this cell does**\n",
        "- Imports standard libraries (pandas/numpy/matplotlib).\n",
        "- Detects the repo root (`REPO_ROOT`) and adds it to `sys.path` so local imports work.\n",
        "- Imports `ElasticityDataPrep` and `PrepConfig` from `data_prep.py`.\n",
        "- Creates a `results/` directory for notebook outputs.\n",
        "\n",
        "**Why we do it**\n",
        "- Notebooks can be run from different working directories; this makes imports and paths reliable.\n",
        "- Ensures exports (CSV artifacts) have a consistent destination.\n",
        "\n",
        "**What to look for**\n",
        "- Printed `repo root:` points to the project root (not `notebooks/`).\n",
        "- `results/ directory:` exists and is writable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Notebook environment ready\n",
            "repo root: C:\\repos\\tr-price-elasticity-bayesian\n",
            "results/ directory: C:\\repos\\tr-price-elasticity-bayesian\\results\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure repo root is on PYTHONPATH so imports work regardless of notebook CWD\n",
        "REPO_ROOT = Path.cwd()\n",
        "if (REPO_ROOT / \"data_prep.py\").exists() is False:\n",
        "    # If we're running from notebooks/, go one level up\n",
        "    REPO_ROOT = Path(__file__).resolve().parents[1] if \"__file__\" in globals() else Path.cwd().parents[0]\n",
        "\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "from data_prep import ElasticityDataPrep, PrepConfig\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "pd.set_option(\"display.width\", 180)\n",
        "\n",
        "# Where outputs from this notebook will go\n",
        "RESULTS_DIR = REPO_ROOT / \"results\"\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Notebook environment ready\")\n",
        "print(\"repo root:\", REPO_ROOT.resolve())\n",
        "print(\"results/ directory:\", RESULTS_DIR.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform raw retailer CSVs into model-ready features\n",
        "\n",
        "**What this cell does**\n",
        "- Sets input file paths for BJ’s / Sam’s / Costco (Costco is optional).\n",
        "- Loads retailer-specific parsing contracts from `config_template.yaml` (important for Costco CRX schema).\n",
        "- Builds a `PrepConfig` (seasonality, promotions, trend, base-vs-promo split, Volume Sales DV rules).\n",
        "- Runs `ElasticityDataPrep.transform(...)` to produce the final prepared dataframe `df`.\n",
        "\n",
        "**Why we do it**\n",
        "- This is the “single source of truth” transformation pipeline used by modeling scripts.\n",
        "- Centralizing config here helps you debug data issues *before* running expensive Bayesian sampling.\n",
        "\n",
        "**What to look for**\n",
        "- The logs show each retailer loads successfully.\n",
        "- `df.shape` looks reasonable (not tiny / not zero rows).\n",
        "- No warnings about missing critical columns (sales, price, date) unless expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BJS_PATH: c:\\repos\\tr-price-elasticity-bayesian\\data\\bjs.csv\n",
            "SAMS_PATH: c:\\repos\\tr-price-elasticity-bayesian\\data\\sams.csv\n",
            "COSTCO_PATH: c:\\repos\\tr-price-elasticity-bayesian\\data\\costco.csv\n",
            "Loaded retailer_data_contracts from: c:\\repos\\tr-price-elasticity-bayesian\\config_template.yaml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STARTING DATA TRANSFORMATION\n",
            "================================================================================\n",
            "\n",
            "Step 1: Loading data...\n",
            "  Loading BJ's... (skiprows=2)\n",
            "  Loading Sam's Club... (skiprows=2)\n",
            "  Loading Costco... (skiprows=1)\n",
            "    Renamed 'Item' → 'Product'\n",
            "  Loaded 2886 rows\n",
            "\n",
            "Step 2: Cleaning data...\n",
            "    BJ's: Avg_Price from 'Dollar Sales' / 'Unit Sales'\n",
            "    Costco: Avg_Price from column 'Avg Net Price'\n",
            "    Costco: Volume Sales computed as Unit Sales x 2.0\n",
            "    BJ's: Base_Price from 'Base Dollar Sales / Base Unit Sales'\n",
            "    Costco: Base price fallback applied for 1 rows where Non Promoted Units < 500\n",
            "    Costco: Base_Price from 'Non Promoted Dollars / Non Promoted Units'\n",
            "  Cleaned to 1115 rows\n",
            "\n",
            "Step 3: Creating features...\n",
            "  Final: 479 rows x 21 columns\n",
            "\n",
            "Step 4: Validating...\n",
            "  ✓ Validation passed\n",
            "\n",
            "================================================================================\n",
            "✓ TRANSFORMATION COMPLETE\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(479, 21)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ---- Configure paths (edit if your filenames differ) ----\n",
        "# IMPORTANT: use absolute paths anchored at REPO_ROOT so this works even if Jupyter's CWD is notebooks/\n",
        "BJS_PATH = str(REPO_ROOT / \"data\" / \"bjs.csv\")\n",
        "SAMS_PATH = str(REPO_ROOT / \"data\" / \"sams.csv\")\n",
        "\n",
        "# Auto-include Costco if the file exists (you can also hardcode a path string here)\n",
        "_default_costco = REPO_ROOT / \"data\" / \"costco.csv\"\n",
        "COSTCO_PATH = str(_default_costco) if _default_costco.exists() else None\n",
        "\n",
        "print(\"BJS_PATH:\", BJS_PATH)\n",
        "print(\"SAMS_PATH:\", SAMS_PATH)\n",
        "print(\"COSTCO_PATH:\", COSTCO_PATH)\n",
        "\n",
        "# Fail fast with a clear error if required inputs are missing\n",
        "for p in [BJS_PATH, SAMS_PATH] + ([COSTCO_PATH] if COSTCO_PATH else []):\n",
        "    if p and not Path(p).exists():\n",
        "        raise FileNotFoundError(f\"Missing input file: {p}\")\n",
        "\n",
        "# ---- Load runtime retailer contracts from config_template.yaml (recommended) ----\n",
        "# These contracts are what allow Costco CRX (different schema) to be parsed correctly.\n",
        "retailer_data_contracts = None\n",
        "try:\n",
        "    import yaml\n",
        "\n",
        "    cfg_path = REPO_ROOT / \"config_template.yaml\"\n",
        "    if cfg_path.exists():\n",
        "        with open(cfg_path, \"r\") as f:\n",
        "            cfg_yaml = yaml.safe_load(f) or {}\n",
        "        retailer_data_contracts = (cfg_yaml.get(\"data\") or {}).get(\"retailer_data_contracts\")\n",
        "        print(\"Loaded retailer_data_contracts from:\", cfg_path)\n",
        "    else:\n",
        "        print(\"WARNING: config_template.yaml not found; proceeding without retailer_data_contracts\")\n",
        "except Exception as e:\n",
        "    print(\"WARNING: could not load retailer_data_contracts (continuing without it). Error:\", e)\n",
        "\n",
        "# ---- Configure data preparation ----\n",
        "# Use retailer_filter=\"All\" to keep retailers separate (needed for hierarchical model).\n",
        "# Use retailer_filter=\"Overall\" to combine into one pooled dataset.\n",
        "\n",
        "retailers_cfg = {\n",
        "    \"BJs\": {\"has_promo\": True, \"has_competitor\": True},\n",
        "    \"Sams\": {\"has_promo\": True, \"has_competitor\": True},\n",
        "}\n",
        "\n",
        "# Costco CRX typically has promo depth inputs but does not include Private Label rows for cross-price.\n",
        "if COSTCO_PATH is not None:\n",
        "    retailers_cfg[\"Costco\"] = {\"has_promo\": True, \"has_competitor\": False}\n",
        "\n",
        "cfg = PrepConfig(\n",
        "    retailer_filter=\"All\",\n",
        "    include_seasonality=True,\n",
        "    include_promotions=True,\n",
        "    include_time_trend=True,\n",
        "\n",
        "    # V2: enable base vs promo separation (default-on in the library)\n",
        "    separate_base_promo=True,\n",
        "\n",
        "    # Base price estimation guardrails (used if base sales cols are missing/undefined)\n",
        "    base_price_proxy_window=8,\n",
        "    base_price_imputed_warn_threshold=0.30,\n",
        "\n",
        "    # Dependent variable rule: always model Volume Sales.\n",
        "    # If a retailer file is missing `Volume Sales`, data prep computes:\n",
        "    #   Volume Sales = Unit Sales × factor\n",
        "    # Costco CRX commonly needs this.\n",
        "    volume_sales_factor_by_retailer={\"Costco\": 2.0},\n",
        "\n",
        "    retailers=retailers_cfg,\n",
        "    retailer_data_contracts=retailer_data_contracts,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "prep = ElasticityDataPrep(cfg)\n",
        "\n",
        "df = prep.transform(\n",
        "    bjs_path=BJS_PATH,\n",
        "    sams_path=SAMS_PATH,\n",
        "    costco_path=COSTCO_PATH,\n",
        ")\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick inspection (preview + column inventory)\n",
        "\n",
        "**What this cell does**\n",
        "- Displays the first ~10 rows of the prepared dataframe.\n",
        "- Prints the full sorted list of columns.\n",
        "\n",
        "**Why we do it**\n",
        "- Ensures the transformation produced the expected schema before we run deeper checks.\n",
        "- Helps you quickly spot obvious parsing problems (bad dates, wrong retailer labels, missing sales/price columns).\n",
        "\n",
        "**What to look for**\n",
        "- `Date` parses correctly, `Retailer` values are clean, and sales/price columns are populated.\n",
        "- Expected columns exist (e.g., `Volume_Sales_SI`, `Price_SI`, log columns, promo/base columns if enabled).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Quick inspection ----\n",
        "display(df.head(10))\n",
        "\n",
        "# Columns\n",
        "cols = sorted(df.columns.tolist())\n",
        "print(f\"Columns ({len(cols)}):\")\n",
        "print(cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retailer breakdown (coverage + date range)\n",
        "\n",
        "**What this cell does**\n",
        "- Counts rows per `Retailer`.\n",
        "- Shows the min/max `Date` per retailer.\n",
        "\n",
        "**Why we do it**\n",
        "- Confirms you actually loaded the retailers you think you did.\n",
        "- Detects partial files (e.g., one retailer missing weeks or a truncated extract).\n",
        "\n",
        "**What to look for**\n",
        "- Row counts are in the expected ballpark and not wildly imbalanced without explanation.\n",
        "- Date ranges overlap enough for comparisons; watch for a retailer that starts/ends much earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Retailer breakdown (if applicable) ----\n",
        "if \"Retailer\" in df.columns:\n",
        "    display(df[\"Retailer\"].value_counts())\n",
        "    display(df.groupby(\"Retailer\").agg(\n",
        "        n_rows=(\"Date\", \"size\"),\n",
        "        min_date=(\"Date\", \"min\"),\n",
        "        max_date=(\"Date\", \"max\"),\n",
        "    ).sort_values(\"n_rows\", ascending=False))\n",
        "else:\n",
        "    print(\"No Retailer column (likely retailer_filter='Overall' or single-retailer filtering).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numeric sanity checks (prices, sales, logs, missingness)\n",
        "\n",
        "**What this cell does**\n",
        "- Runs `describe()` on key numeric columns (sales, price, logs, promo intensity).\n",
        "- Prints minimum values for quick “is anything zero/negative?” checks.\n",
        "- Computes missing rates for the same key columns.\n",
        "\n",
        "**Why we do it**\n",
        "- Prevents subtle downstream failures (e.g., log of non-positive numbers, missing features).\n",
        "- Surfaces retailer-specific issues early (bad parsing, wrong units, missing columns).\n",
        "\n",
        "**What to look for**\n",
        "- `Price_SI` and `Volume_Sales_SI` should be strictly > 0 (or at least non-negative with clear handling).\n",
        "- Logs should be finite (no `inf`, no `-inf`, no `NaN`).\n",
        "- Missingness should match expectations (e.g., competitor features missing for Costco if disabled).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Numeric sanity checks ----\n",
        "key_cols = [\n",
        "    \"Volume_Sales_SI\",\"Volume_Sales_PL\",\n",
        "    \"Price_SI\",\"Price_PL\",\n",
        "    \"Log_Volume_Sales_SI\",\"Log_Price_SI\",\"Log_Price_PL\",\n",
        "    \"Promo_Intensity_SI\",\n",
        "    \"Week_Number\",\n",
        "    \"has_promo\",\"has_competitor\",\n",
        "]\n",
        "key_cols = [c for c in key_cols if c in df.columns]\n",
        "\n",
        "display(df[key_cols].describe().T)\n",
        "\n",
        "# Quick checks for suspicious values\n",
        "if \"Price_SI\" in df.columns:\n",
        "    print(\"Min Price_SI:\", df[\"Price_SI\"].min())\n",
        "if \"Volume_Sales_SI\" in df.columns:\n",
        "    print(\"Min Volume_Sales_SI:\", df[\"Volume_Sales_SI\"].min())\n",
        "\n",
        "# Missingness report\n",
        "missing_rate = df[key_cols].isna().mean().sort_values(ascending=False)\n",
        "display(missing_rate.to_frame(\"missing_rate\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Availability flags (Costco-style missing features)\n",
        "\n",
        "**What this cell does**\n",
        "- Aggregates `has_promo` and `has_competitor` by retailer.\n",
        "- Optionally summarizes `Promo_Intensity_SI` by retailer.\n",
        "\n",
        "**Why we do it**\n",
        "- Some retailers (often Costco) legitimately lack competitor / private label features.\n",
        "- These flags are used for masking so missing features don’t silently become misleading zeros.\n",
        "\n",
        "**What to look for**\n",
        "- For Costco, you typically expect `has_competitor` ≈ 0 (or `False`) if competitor is disabled.\n",
        "- Promo intensity should not be all-NaN if promotions are enabled and data supports it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Availability flags sanity (Costco-style missing features) ----\n",
        "if \"Retailer\" in df.columns and \"has_promo\" in df.columns:\n",
        "    display(df.groupby(\"Retailer\")[[\"has_promo\",\"has_competitor\"]].mean())\n",
        "\n",
        "    if \"Promo_Intensity_SI\" in df.columns:\n",
        "        display(df.groupby(\"Retailer\")[\"Promo_Intensity_SI\"].agg([\"mean\",\"min\",\"max\"]))\n",
        "else:\n",
        "    print(\"No availability flags found (expected if you did not pass cfg.retailers).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick plots (shape-of-the-world check)\n",
        "\n",
        "**What this cell does**\n",
        "- Plots `Volume_Sales_SI` over time.\n",
        "- Plots a scatter of `Log_Price_SI` vs `Log_Volume_Sales_SI`.\n",
        "\n",
        "**Why we do it**\n",
        "- Time-series plots catch missing weeks, step changes, and bad parsing.\n",
        "- The scatter should show a *rough* downward relationship (higher price → lower sales), even if noisy.\n",
        "\n",
        "**What to look for**\n",
        "- No long flatlines at 0, no huge spikes driven by one bad row, and dates are monotonic.\n",
        "- Scatter: check for vertical lines (constant price) or extreme outliers (bad log inputs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Basic plots (interim confidence) ----\n",
        "df_plot = df.sort_values(\"Date\")\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "plt.plot(df_plot[\"Date\"], df_plot[\"Volume_Sales_SI\"], linewidth=1)\n",
        "plt.title(\"Sparkling Ice Volume Sales over time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Volume_Sales_SI\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(df_plot[\"Log_Price_SI\"], df_plot[\"Log_Volume_Sales_SI\"], s=12, alpha=0.4)\n",
        "plt.title(\"Log Sales vs Log Own Price\")\n",
        "plt.xlabel(\"Log_Price_SI\")\n",
        "plt.ylabel(\"Log_Volume_Sales_SI\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export prepared data (audit trail)\n",
        "\n",
        "**What this cell does**\n",
        "- Writes the transformed, model-ready dataset to `results/prepared_data_from_notebook.csv`.\n",
        "\n",
        "**Why we do it**\n",
        "- Creates an auditable artifact you can share/inspect outside the notebook.\n",
        "- Makes it easy to diff outputs across pipeline changes.\n",
        "\n",
        "**What to look for**\n",
        "- The printed path exists and the file size looks reasonable (not 0 bytes).\n",
        "- Open the CSV and spot-check a few rows for dates, retailers, sales, and prices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Export prepared data for auditability ----\n",
        "out_path = RESULTS_DIR / \"prepared_data_from_notebook.csv\"\n",
        "df.to_csv(out_path, index=False)\n",
        "print(\"Wrote:\", out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## V2 feature checks (base price + promo depth)\n",
        "\n",
        "**What this cell does**\n",
        "- Looks for V2 columns like `Base_Price_SI`, `Log_Base_Price_SI`, and `Promo_Depth_SI`.\n",
        "- Summarizes their distributions and plots a histogram of promo depth.\n",
        "\n",
        "**Why we do it**\n",
        "- Confirms the base-vs-promo decomposition is present and numerically reasonable.\n",
        "- Validates promo depth sign/scale so the model doesn’t learn from bad discount features.\n",
        "\n",
        "**What to look for**\n",
        "- `Promo_Depth_SI` should be near 0 when not on deal and **negative** during discounts.\n",
        "- Extreme values (e.g., < -0.8 or > 0.2) usually indicate bad base price estimation or data issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- V2 feature checks: base price + promo depth ----\n",
        "v2_cols = [\"Base_Price_SI\", \"Log_Base_Price_SI\", \"Promo_Depth_SI\"]\n",
        "v2_cols = [c for c in v2_cols if c in df.columns]\n",
        "\n",
        "if v2_cols:\n",
        "    display(df[v2_cols].describe().T)\n",
        "\n",
        "    # Promo depth interpretation: values are relative price change vs base.\n",
        "    #  - 0.00 means no discount (avg price == base price)\n",
        "    #  - negative means discounted (e.g., -0.10 ≈ 10% off)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.hist(df[\"Promo_Depth_SI\"], bins=50, alpha=0.75, edgecolor=\"black\")\n",
        "    plt.title(\"Promo_Depth_SI distribution (negative = discount)\")\n",
        "    plt.xlabel(\"Promo_Depth_SI\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.grid(alpha=0.25)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"V2 columns not found. Check that PrepConfig(separate_base_promo=True) and input columns exist.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Case Validation Suite (assert-based)\n",
        "\n",
        "This section converts `Test_Case_Instructions.md` into **executable notebook validations**.\n",
        "\n",
        "- These checks are designed to fail fast with clear messages.\n",
        "- They reuse the already-computed objects from above:\n",
        "  - `prep` (contains `prep.raw_data`, `prep.cleaned_data`, `prep.final_data`)\n",
        "  - `df` (the final model-ready dataframe)\n",
        "  - `BJS_PATH`, `SAMS_PATH`, `COSTCO_PATH`\n",
        "\n",
        "If a test fails, fix the underlying pipeline (usually in `data_prep.py` or `config_template.yaml`) and re-run the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shared helpers for notebook-style tests\n",
        "\n",
        "**What this cell does**\n",
        "- Defines small assertion helpers with better error messages than raw `assert`.\n",
        "- Defines helpers for loading raw CSVs with the correct `skiprows`.\n",
        "\n",
        "**Why we do it**\n",
        "- Keeps the test group cells focused on the business logic checks.\n",
        "- Makes failures easier to debug (prints actual vs expected and relevant sample rows).\n",
        "\n",
        "**What to look for**\n",
        "- No output is expected when this cell runs; it only defines helper functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "import re\n",
        "from typing import Optional, Iterable\n",
        "\n",
        "\n",
        "def _fmt(x):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return x\n",
        "\n",
        "\n",
        "def assert_true(cond: bool, msg: str):\n",
        "    assert bool(cond), msg\n",
        "\n",
        "\n",
        "def assert_eq(actual, expected, msg: str = \"\"):\n",
        "    assert actual == expected, msg or f\"Expected {expected!r}, got {actual!r}\"\n",
        "\n",
        "\n",
        "def assert_between(x, lo: float, hi: float, msg: str = \"\"):\n",
        "    xf = float(x)\n",
        "    assert lo <= xf <= hi, msg or f\"Expected {xf} to be between [{lo}, {hi}]\"\n",
        "\n",
        "\n",
        "def assert_all_between(series: pd.Series, lo: float, hi: float, label: str):\n",
        "    bad = series[(series < lo) | (series > hi) | series.isna() | ~np.isfinite(series)]\n",
        "    assert_true(bad.empty, f\"{label}: found {len(bad)} values outside [{lo}, {hi}] or non-finite. Sample:\\n{bad.head(10)}\")\n",
        "\n",
        "\n",
        "def assert_approx(actual: float, expected: float, tol: float, msg: str = \"\"):\n",
        "    a = float(actual)\n",
        "    e = float(expected)\n",
        "    assert abs(a - e) <= tol, msg or f\"Expected {a} ≈ {e} (tol={tol}), diff={a-e}\"\n",
        "\n",
        "\n",
        "def load_raw_csv(path: str, *, skiprows: int) -> pd.DataFrame:\n",
        "    df0 = pd.read_csv(path, skiprows=skiprows)\n",
        "    return df0\n",
        "\n",
        "\n",
        "def _subset_final(df_final: pd.DataFrame, retailer: str) -> pd.DataFrame:\n",
        "    return df_final[df_final[\"Retailer\"] == retailer].copy()\n",
        "\n",
        "\n",
        "def _subset_cleaned(cleaned_long: pd.DataFrame, retailer: str, product_short: Optional[str] = None) -> pd.DataFrame:\n",
        "    out = cleaned_long[cleaned_long[\"Retailer\"] == retailer].copy()\n",
        "    if product_short is not None:\n",
        "        out = out[out[\"Product_Short\"] == product_short].copy()\n",
        "    return out\n",
        "\n",
        "\n",
        "print(\"Test helpers loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 1 — Raw File Loading\n",
        "\n",
        "**What this cell does**\n",
        "- Loads each raw CSV with the expected `skiprows`.\n",
        "- Asserts raw schema/row counts match the known contracts.\n",
        "- Validates Costco’s `Item` → `Product` rename in the pipeline loader.\n",
        "\n",
        "**Why we do it**\n",
        "- Confirms we’re reading the right files with the right header handling.\n",
        "- Prevents subtle downstream issues caused by the wrong `skiprows` or missing columns.\n",
        "\n",
        "**What to look for**\n",
        "- No output except the final “passed” line.\n",
        "- If this fails, the issue is usually wrong file path, wrong `skiprows`, or a changed source extract schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1.1 — BJ's loads with correct schema\n",
        "bjs_raw = load_raw_csv(BJS_PATH, skiprows=2)\n",
        "assert_eq(bjs_raw.shape[1], 74, \"BJ's expected 74 columns when loaded with skiprows=2\")\n",
        "for c in [\"Product\", \"Time\", \"Base Dollar Sales\", \"Base Unit Sales\", \"Volume Sales\"]:\n",
        "    assert_true(c in bjs_raw.columns, f\"BJ's missing required column: {c}\")\n",
        "assert_eq(len(bjs_raw), 477, \"BJ's expected 477 raw rows\")\n",
        "\n",
        "# Test 1.2 — Sam's loads with correct schema\n",
        "sams_raw = load_raw_csv(SAMS_PATH, skiprows=2)\n",
        "assert_eq(sams_raw.shape[1], 74, \"Sam's expected 74 columns when loaded with skiprows=2\")\n",
        "for c in [\"Product\", \"Time\"]:\n",
        "    assert_true(c in sams_raw.columns, f\"Sam's missing required column: {c}\")\n",
        "assert_true(list(sams_raw.columns) == list(bjs_raw.columns), \"Sam's columns should exactly match BJ's (Circana schema)\")\n",
        "assert_eq(len(sams_raw), 477, \"Sam's expected 477 raw rows\")\n",
        "\n",
        "# Test 1.3 — Costco loads with correct schema\n",
        "assert_true(COSTCO_PATH is not None, \"COSTCO_PATH is None but tests expect costco.csv to be present\")\n",
        "costco_raw = load_raw_csv(COSTCO_PATH, skiprows=1)\n",
        "assert_eq(costco_raw.shape[1], 23, \"Costco expected 23 columns when loaded with skiprows=1\")\n",
        "for c in [\"Item\", \"Avg Net Price\", \"Non Promoted Dollars\", \"Non Promoted Units\", \"Average Price per Unit\"]:\n",
        "    assert_true(c in costco_raw.columns, f\"Costco missing required column: {c}\")\n",
        "for c in [\"Volume Sales\", \"Base Dollar Sales\"]:\n",
        "    assert_true(c not in costco_raw.columns, f\"Costco should NOT have column: {c}\")\n",
        "assert_eq(len(costco_raw), 1932, \"Costco expected 1932 raw rows\")\n",
        "\n",
        "# Test 1.4 — Costco Item column renamed to Product after load\n",
        "costco_loaded = prep._load_single_retailer(COSTCO_PATH, \"Costco\")\n",
        "assert_true(\"Product\" in costco_loaded.columns, \"Costco loader should rename Item → Product\")\n",
        "assert_true(\"Item\" not in costco_loaded.columns, \"Costco loader output should not retain Item after rename\")\n",
        "assert_true((costco_loaded[\"Retailer\"] == \"Costco\").all(), \"Costco loader must set Retailer='Costco' on all rows\")\n",
        "\n",
        "print(\"✓ Test Group 1 passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 2 — Product Filtering\n",
        "\n",
        "**What this cell does**\n",
        "- Validates the pipeline’s brand/competitor filtering results in the expected row counts.\n",
        "- Confirms Costco uses the more specific `sparkling ice core` filter so UPC-level rows are excluded.\n",
        "\n",
        "**Why we do it**\n",
        "- Product filtering determines which rows survive into the model dataset.\n",
        "- Costco is especially sensitive: using `sparkling ice` alone would incorrectly include UPC rows.\n",
        "\n",
        "**What to look for**\n",
        "- BJ’s/Sam’s: 159 Sparkling Ice + 318 Private Label rows each (477 total per retailer in the cleaned long table).\n",
        "- Costco: exactly 161 Sparkling Ice rows, 0 Private Label rows, and no rows starting with `ITEM `.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleaned = prep.cleaned_data\n",
        "\n",
        "# Test 2.1 — BJ's brand filtering\n",
        "bjs = _subset_cleaned(cleaned, \"BJ's\")\n",
        "counts_bjs = bjs[\"Product_Short\"].value_counts(dropna=False)\n",
        "assert_eq(int(counts_bjs.get(\"Sparkling Ice\", 0)), 159, f\"BJ's Sparkling Ice rows expected 159, got {int(counts_bjs.get('Sparkling Ice',0))}\")\n",
        "assert_eq(int(counts_bjs.get(\"Private Label\", 0)), 318, f\"BJ's Private Label rows expected 318, got {int(counts_bjs.get('Private Label',0))}\")\n",
        "assert_eq(len(bjs), 477, f\"BJ's total filtered rows expected 477, got {len(bjs)}\")\n",
        "\n",
        "# Test 2.2 — Sam's brand filtering\n",
        "sams = _subset_cleaned(cleaned, \"Sam's Club\")\n",
        "counts_sams = sams[\"Product_Short\"].value_counts(dropna=False)\n",
        "assert_eq(int(counts_sams.get(\"Sparkling Ice\", 0)), 159, f\"Sam's Sparkling Ice rows expected 159, got {int(counts_sams.get('Sparkling Ice',0))}\")\n",
        "assert_eq(int(counts_sams.get(\"Private Label\", 0)), 318, f\"Sam's Private Label rows expected 318, got {int(counts_sams.get('Private Label',0))}\")\n",
        "assert_eq(len(sams), 477, f\"Sam's total filtered rows expected 477, got {len(sams)}\")\n",
        "\n",
        "# Test 2.3 — Costco brand filtering with \"sparkling ice core\"\n",
        "# Validate raw-string matching counts first (using raw Costco file with the original 'Item' column)\n",
        "costco_raw_lower = costco_raw.copy()\n",
        "costco_raw_lower[\"Item\"] = costco_raw_lower[\"Item\"].astype(str)\n",
        "count_sparkling_ice = int(costco_raw_lower[\"Item\"].str.lower().str.contains(\"sparkling ice\", na=False).sum())\n",
        "count_sparkling_ice_core = int(costco_raw_lower[\"Item\"].str.lower().str.contains(\"sparkling ice core\", na=False).sum())\n",
        "assert_eq(count_sparkling_ice, 1932, f\"Costco rows matching 'sparkling ice' expected 1932, got {count_sparkling_ice}\")\n",
        "assert_eq(count_sparkling_ice_core, 161, f\"Costco rows matching 'sparkling ice core' expected 161, got {count_sparkling_ice_core}\")\n",
        "\n",
        "costco = _subset_cleaned(cleaned, \"Costco\")\n",
        "counts_costco = costco[\"Product_Short\"].value_counts(dropna=False)\n",
        "assert_eq(int(counts_costco.get(\"Sparkling Ice\", 0)), 161, f\"Costco Sparkling Ice rows expected 161, got {int(counts_costco.get('Sparkling Ice',0))}\")\n",
        "assert_eq(int(counts_costco.get(\"Private Label\", 0)), 0, f\"Costco Private Label rows expected 0, got {int(counts_costco.get('Private Label',0))}\")\n",
        "\n",
        "# All Costco rows should be the brand aggregate item, not the UPC-level 'ITEM ...' rows\n",
        "expected_item = \"Sparkling Ice Core 17oz 24ct 2023 through 2025 Items\"\n",
        "assert_true((costco[\"Product\"].astype(str) == expected_item).all(), \"Costco filtered rows should all be the brand aggregate Item string\")\n",
        "\n",
        "# Test 2.4 — Costco UPC rows excluded\n",
        "assert_eq(int(costco[\"Product\"].astype(str).str.startswith(\"ITEM \", na=False).sum()), 0, \"No Costco rows starting with 'ITEM ' should survive filtering\")\n",
        "\n",
        "print(\"✓ Test Group 2 passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 3 — Date Parsing\n",
        "\n",
        "**What this cell does**\n",
        "- Tests the retailer-specific date parsing logic on known example strings.\n",
        "- Asserts there are no `NaT` dates after parsing.\n",
        "- Checks expected min/max date ranges and expected number of weekly observations.\n",
        "- Validates that retailers overlap in time for joint modeling.\n",
        "\n",
        "**Why we do it**\n",
        "- Incorrect date parsing silently breaks time features, seasonality, and week alignment.\n",
        "\n",
        "**What to look for**\n",
        "- Exact example inputs parse to the expected timestamps.\n",
        "- BJ’s/Sam’s span `2023-01-08` to `2026-01-18` and have 159 unique weeks for Sparkling Ice.\n",
        "- Costco spans `2023-01-01` to `2026-01-25` and has 161 unique weeks.\n",
        "- Overlap across all three retailers is at least 150 weeks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3.1 — BJ's date parsing (example string)\n",
        "ex_bjs = pd.Series([\"Week Ending 01-08-23\"]) \n",
        "parsed_bjs = prep._parse_date_for_retailer(ex_bjs, \"BJ's\").iloc[0]\n",
        "assert_eq(parsed_bjs, pd.Timestamp(\"2023-01-08\"), \"BJ's example date should parse to 2023-01-08\")\n",
        "\n",
        "# Test 3.3 — Costco date parsing (example string)\n",
        "ex_costco = pd.Series([\"1 week ending 01-08-2023\"]) \n",
        "parsed_costco = prep._parse_date_for_retailer(ex_costco, \"Costco\").iloc[0]\n",
        "assert_eq(parsed_costco, pd.Timestamp(\"2023-01-08\"), \"Costco example date should parse to 2023-01-08\")\n",
        "\n",
        "# Assert no NaT after parsing in cleaned data\n",
        "cleaned = prep.cleaned_data\n",
        "for retailer in [\"BJ's\", \"Sam's Club\", \"Costco\"]:\n",
        "    r = cleaned[cleaned[\"Retailer\"] == retailer]\n",
        "    assert_true(r[\"Date\"].isna().sum() == 0, f\"{retailer}: found NaT/NaN dates after parsing\")\n",
        "\n",
        "# BJ's range + unique weeks for Sparkling Ice\n",
        "bjs_si = _subset_cleaned(cleaned, \"BJ's\", \"Sparkling Ice\")\n",
        "assert_eq(bjs_si[\"Date\"].min(), pd.Timestamp(\"2023-01-08\"), \"BJ's min date mismatch\")\n",
        "assert_eq(bjs_si[\"Date\"].max(), pd.Timestamp(\"2026-01-18\"), \"BJ's max date mismatch\")\n",
        "assert_eq(bjs_si[\"Date\"].nunique(), 159, f\"BJ's expected 159 unique Sparkling Ice dates, got {bjs_si['Date'].nunique()}\")\n",
        "\n",
        "# Sam's range\n",
        "sams_si = _subset_cleaned(cleaned, \"Sam's Club\", \"Sparkling Ice\")\n",
        "assert_eq(sams_si[\"Date\"].min(), pd.Timestamp(\"2023-01-08\"), \"Sam's min date mismatch\")\n",
        "assert_eq(sams_si[\"Date\"].max(), pd.Timestamp(\"2026-01-18\"), \"Sam's max date mismatch\")\n",
        "\n",
        "# Costco range + unique weeks\n",
        "costco_si = _subset_cleaned(cleaned, \"Costco\", \"Sparkling Ice\")\n",
        "assert_eq(costco_si[\"Date\"].min(), pd.Timestamp(\"2023-01-01\"), \"Costco min date mismatch\")\n",
        "assert_eq(costco_si[\"Date\"].max(), pd.Timestamp(\"2026-01-25\"), \"Costco max date mismatch\")\n",
        "assert_eq(costco_si[\"Date\"].nunique(), 161, f\"Costco expected 161 unique dates, got {costco_si['Date'].nunique()}\")\n",
        "\n",
        "# Test 3.4 — overlap across all three retailers (use final wide df: one row per retailer-week)\n",
        "d_bjs = set(_subset_final(df, \"BJ's\")[\"Date\"])\n",
        "d_sams = set(_subset_final(df, \"Sam's Club\")[\"Date\"])\n",
        "d_costco = set(_subset_final(df, \"Costco\")[\"Date\"])\n",
        "overlap = d_bjs & d_sams & d_costco\n",
        "assert_true(len(overlap) >= 150, f\"Expected at least 150 overlapping weeks across all retailers, got {len(overlap)}\")\n",
        "\n",
        "print(\"✓ Test Group 3 passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 4 — Price Calculations (CRITICAL)\n",
        "\n",
        "**What this cell does**\n",
        "- Validates how `Avg_Price` (→ `Price_SI`) and `Base_Avg_Price` (→ `Base_Price_SI`) are computed per retailer.\n",
        "- Includes Costco-specific checks that ensure `Avg Net Price` is used (not shelf price) and that base-price fallback works for low NP Units.\n",
        "\n",
        "**Why we do it**\n",
        "- Price variables drive elasticity estimates; a subtle price bug can invalidate the model.\n",
        "\n",
        "**What to look for**\n",
        "- BJ’s/Sam’s: `Avg_Price = Dollar Sales / Unit Sales` and base price uses Circana base columns.\n",
        "- Costco: `Avg_Price = Avg Net Price` and base price uses `Non Promoted Dollars / Non Promoted Units` with fallback when NP Units < 500.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleaned = prep.cleaned_data\n",
        "\n",
        "# Helpers: fetch Sparkling Ice rows for each retailer in the cleaned long table\n",
        "bjs_si = _subset_cleaned(cleaned, \"BJ's\", \"Sparkling Ice\")\n",
        "sams_si = _subset_cleaned(cleaned, \"Sam's Club\", \"Sparkling Ice\")\n",
        "costco_si = _subset_cleaned(cleaned, \"Costco\", \"Sparkling Ice\")\n",
        "\n",
        "# Test 4.1 — BJ's average price = Dollar Sales / Unit Sales\n",
        "bjs_calc = bjs_si[\"Dollar Sales\"].astype(float) / bjs_si[\"Unit Sales\"].replace(0, np.nan).astype(float)\n",
        "max_abs_diff = float((bjs_si[\"Avg_Price\"].astype(float) - bjs_calc).abs().max())\n",
        "assert_true(max_abs_diff < 1e-6, f\"BJ's Avg_Price mismatch vs Dollar/Unit. max_abs_diff={max_abs_diff}\")\n",
        "assert_all_between(bjs_si[\"Avg_Price\"].astype(float), 15.68, 19.67, \"BJ's Avg_Price range\")\n",
        "assert_true(np.isfinite(bjs_si[\"Avg_Price\"].astype(float)).all(), \"BJ's Avg_Price contains non-finite values\")\n",
        "\n",
        "# Test 4.2 — Sam's average price = Dollar Sales / Unit Sales\n",
        "sams_calc = sams_si[\"Dollar Sales\"].astype(float) / sams_si[\"Unit Sales\"].replace(0, np.nan).astype(float)\n",
        "max_abs_diff = float((sams_si[\"Avg_Price\"].astype(float) - sams_calc).abs().max())\n",
        "assert_true(max_abs_diff < 1e-6, f\"Sam's Avg_Price mismatch vs Dollar/Unit. max_abs_diff={max_abs_diff}\")\n",
        "assert_all_between(sams_si[\"Avg_Price\"].astype(float), 12.32, 17.90, \"Sam's Avg_Price range\")\n",
        "\n",
        "# Test 4.3 — Costco average price = Avg Net Price (NOT Dollar Sales / Unit Sales)\n",
        "max_abs_diff = float((costco_si[\"Avg_Price\"].astype(float) - costco_si[\"Avg Net Price\"].astype(float)).abs().max())\n",
        "assert_true(max_abs_diff < 1e-6, f\"Costco Avg_Price should equal Avg Net Price. max_abs_diff={max_abs_diff}\")\n",
        "assert_all_between(costco_si[\"Avg_Price\"].astype(float), 11.71, 18.05, \"Costco Avg_Price range\")\n",
        "\n",
        "# Promo-week spot check (02-05-2023): net vs shelf price\n",
        "row_02052023 = costco_si.loc[costco_si[\"Date\"] == pd.Timestamp(\"2023-02-05\")]\n",
        "assert_true(len(row_02052023) == 1, f\"Expected exactly 1 Costco row for 2023-02-05, got {len(row_02052023)}\")\n",
        "row_02052023 = row_02052023.iloc[0]\n",
        "net_price = float(row_02052023[\"Avg Net Price\"])\n",
        "shelf_price = float(row_02052023[\"Dollar Sales\"]) / float(row_02052023[\"Unit Sales\"])\n",
        "assert_approx(net_price, 12.06, tol=0.10, msg=f\"Costco 2023-02-05 Avg Net Price unexpected: {net_price}\")\n",
        "assert_approx(shelf_price, 16.01, tol=0.20, msg=f\"Costco 2023-02-05 shelf price (Dollar/Unit) unexpected: {shelf_price}\")\n",
        "assert_approx(float(row_02052023[\"Avg_Price\"]), 12.06, tol=0.10, msg=\"Pipeline must use Avg Net Price (~12.06), not shelf price (~16.01)\")\n",
        "\n",
        "# Test 4.4 — BJ's base price = Base Dollar Sales / Base Unit Sales\n",
        "bjs_base_calc = bjs_si[\"Base Dollar Sales\"].astype(float) / bjs_si[\"Base Unit Sales\"].replace(0, np.nan).astype(float)\n",
        "max_abs_diff = float((bjs_si[\"Base_Avg_Price\"].astype(float) - bjs_base_calc).abs().max())\n",
        "assert_true(max_abs_diff < 1e-6, f\"BJ's Base_Avg_Price mismatch. max_abs_diff={max_abs_diff}\")\n",
        "assert_all_between(bjs_si[\"Base_Avg_Price\"].astype(float), 16.82, 19.68, \"BJ's Base price range\")\n",
        "assert_true(bjs_si[\"Base_Avg_Price\"].notna().all(), \"BJ's Base_Avg_Price contains NaN\")\n",
        "\n",
        "# Test 4.5 — Sam's base price = Base Dollar Sales / Base Unit Sales\n",
        "sams_base_calc = sams_si[\"Base Dollar Sales\"].astype(float) / sams_si[\"Base Unit Sales\"].replace(0, np.nan).astype(float)\n",
        "max_abs_diff = float((sams_si[\"Base_Avg_Price\"].astype(float) - sams_base_calc).abs().max())\n",
        "assert_true(max_abs_diff < 1e-6, f\"Sam's Base_Avg_Price mismatch. max_abs_diff={max_abs_diff}\")\n",
        "assert_all_between(sams_si[\"Base_Avg_Price\"].astype(float), 15.53, 17.90, \"Sam's Base price range\")\n",
        "\n",
        "# Test 4.6 — Costco base price = Non Promoted Dollars / Non Promoted Units (with fallback)\n",
        "np_units = costco_si[\"Non Promoted Units\"].astype(float)\n",
        "np_base_calc = costco_si[\"Non Promoted Dollars\"].astype(float) / np_units.replace(0, np.nan)\n",
        "base_price = costco_si[\"Base_Avg_Price\"].astype(float)\n",
        "\n",
        "low_units_mask = np_units < 500\n",
        "fallback_weeks = int(low_units_mask.sum())\n",
        "\n",
        "# For normal weeks (NP Units >= 500), base price should match NP$/NP Units\n",
        "normal_mask = ~low_units_mask\n",
        "max_abs_diff = float((base_price[normal_mask] - np_base_calc[normal_mask]).abs().max())\n",
        "assert_true(max_abs_diff < 1e-6, f\"Costco Base_Avg_Price mismatch on normal weeks. max_abs_diff={max_abs_diff}\")\n",
        "\n",
        "# For low-unit weeks, base price should equal Average Price per Unit\n",
        "if fallback_weeks > 0:\n",
        "    max_abs_diff_fb = float((base_price[low_units_mask] - costco_si.loc[low_units_mask, \"Average Price per Unit\"].astype(float)).abs().max())\n",
        "    assert_true(max_abs_diff_fb < 1e-6, f\"Costco base-price fallback mismatch. max_abs_diff={max_abs_diff_fb}\")\n",
        "    assert_all_between(base_price[low_units_mask], 15.50, 18.50, \"Costco fallback base-price range\")\n",
        "\n",
        "assert_all_between(base_price, 15.74, 18.05, \"Costco Base_Avg_Price range\")\n",
        "assert_true(base_price.notna().all(), \"Costco Base_Avg_Price contains NaN\")\n",
        "\n",
        "# Test 4.7 — Costco base price is NOT Average Price per Unit for normal weeks (values should be close but not identical)\n",
        "# We assert that at least one normal week differs by > 0.001 to avoid false passes.\n",
        "diffs = (base_price[normal_mask] - costco_si.loc[normal_mask, \"Average Price per Unit\"].astype(float)).abs()\n",
        "assert_true((diffs > 0.001).any(), \"Costco normal weeks: Base_Avg_Price should not be identical to Average Price per Unit\")\n",
        "\n",
        "print(f\"✓ Test Group 4 passed (Costco fallback weeks detected: {fallback_weeks})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 5 — Promo Depth\n",
        "\n",
        "**What this cell does**\n",
        "- Validates the promo depth formula and clipping.\n",
        "- Checks retailer-specific promo depth ranges and counts of promo vs non-promo weeks.\n",
        "- Runs a couple of Costco week-specific validations.\n",
        "\n",
        "**Why we do it**\n",
        "- Promo depth is the key V2 feature that separates base vs promotional elasticity.\n",
        "\n",
        "**What to look for**\n",
        "- `Promo_Depth_SI` equals `clip((Price_SI / Base_Price_SI) - 1, [-0.80, 0.50])`.\n",
        "- Retailer ranges match expectations (Sam’s typically deepest discounts).\n",
        "- Costco: ~102 non-promo weeks near 0 and ~59 promo weeks meaningfully negative.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final = df.copy()\n",
        "\n",
        "# Test 5.1 — Promo depth formula + clipping\n",
        "calc_raw = (df_final[\"Price_SI\"].astype(float) / df_final[\"Base_Price_SI\"].astype(float)) - 1.0\n",
        "calc_clipped = calc_raw.clip(-0.80, 0.50)\n",
        "max_abs_diff = float((df_final[\"Promo_Depth_SI\"].astype(float) - calc_clipped).abs().max())\n",
        "assert_true(max_abs_diff < 1e-9, f\"Promo_Depth_SI must equal clipped formula. max_abs_diff={max_abs_diff}\")\n",
        "\n",
        "# Test 5.5 — clipping bounds\n",
        "assert_all_between(df_final[\"Promo_Depth_SI\"].astype(float), -0.80, 0.50, \"Promo_Depth_SI clip bounds\")\n",
        "\n",
        "# Retailer-specific ranges\n",
        "bjs_pd = _subset_final(df_final, \"BJ's\")[\"Promo_Depth_SI\"].astype(float)\n",
        "sams_pd = _subset_final(df_final, \"Sam's Club\")[\"Promo_Depth_SI\"].astype(float)\n",
        "costco_pd = _subset_final(df_final, \"Costco\")[\"Promo_Depth_SI\"].astype(float)\n",
        "\n",
        "assert_all_between(bjs_pd, -0.15, 0.01, \"BJ's Promo_Depth_SI range\")\n",
        "assert_all_between(sams_pd, -0.30, 0.01, \"Sam's Promo_Depth_SI range\")\n",
        "assert_all_between(costco_pd, -0.30, 0.001, \"Costco Promo_Depth_SI range\")\n",
        "\n",
        "# Costco non-promo vs promo week counts\n",
        "costco_nonpromo = int((costco_pd.abs() < 0.01).sum())\n",
        "costco_promo = int((costco_pd < -0.01).sum())\n",
        "# NOTE: counts are dataset-dependent; these expectations match the current `data/costco.csv` in this repo.\n",
        "assert_eq(costco_nonpromo, 108, f\"Costco expected 108 non-promo weeks with |Promo_Depth|<0.01, got {costco_nonpromo}\")\n",
        "assert_eq(costco_promo, 53, f\"Costco expected 53 promo weeks with Promo_Depth<-0.01, got {costco_promo}\")\n",
        "\n",
        "# Week-specific validations\n",
        "row_02052023 = _subset_final(df_final, \"Costco\").loc[lambda d: d[\"Date\"] == pd.Timestamp(\"2023-02-05\")]\n",
        "assert_true(len(row_02052023) == 1, f\"Expected 1 Costco row for 2023-02-05, got {len(row_02052023)}\")\n",
        "row_02052023 = row_02052023.iloc[0]\n",
        "assert_approx(float(row_02052023[\"Promo_Depth_SI\"]), -0.248, tol=0.01, msg=\"Costco 2023-02-05 Promo_Depth_SI unexpected\")\n",
        "\n",
        "row_01012023 = _subset_final(df_final, \"Costco\").loc[lambda d: d[\"Date\"] == pd.Timestamp(\"2023-01-01\")]\n",
        "assert_true(len(row_01012023) == 1, f\"Expected 1 Costco row for 2023-01-01, got {len(row_01012023)}\")\n",
        "row_01012023 = row_01012023.iloc[0]\n",
        "assert_true(abs(float(row_01012023[\"Promo_Depth_SI\"])) < 0.001, \"Costco 2023-01-01 should be non-promo (Promo_Depth near 0)\")\n",
        "\n",
        "print(\"✓ Test Group 5 passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 6 — Volume Sales\n",
        "\n",
        "**What this cell does**\n",
        "- Confirms Volume Sales behavior per retailer:\n",
        "  - BJ’s/Sam’s: use the direct `Volume Sales` column.\n",
        "  - Costco: compute `Volume Sales = Unit Sales × 2.0`.\n",
        "\n",
        "**Why we do it**\n",
        "- The dependent variable is modeled on **Volume Sales**, so this must be correct.\n",
        "\n",
        "**What to look for**\n",
        "- BJ’s/Sam’s ratio `Volume Sales / Unit Sales` is exactly 2.0 for all rows.\n",
        "- Costco `Volume_Sales_SI` matches `Unit Sales × 2.0` and falls in the expected range.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleaned = prep.cleaned_data\n",
        "\n",
        "# Test 6.1 — BJ's Volume Sales = direct column; ratio Volume/Unit = 2.0\n",
        "bjs_all = _subset_cleaned(cleaned, \"BJ's\")\n",
        "ratio_bjs = (bjs_all[\"Volume Sales\"].astype(float) / bjs_all[\"Unit Sales\"].astype(float)).round(4)\n",
        "assert_true((ratio_bjs == 2.0000).all(), f\"BJ's expected Volume/Unit ratio == 2.0000 for all rows. Sample:\\n{ratio_bjs.value_counts().head()}\")\n",
        "\n",
        "# Test 6.2 — Sam's Volume Sales = direct column; ratio = 2.0\n",
        "sams_all = _subset_cleaned(cleaned, \"Sam's Club\")\n",
        "ratio_sams = (sams_all[\"Volume Sales\"].astype(float) / sams_all[\"Unit Sales\"].astype(float)).round(4)\n",
        "assert_true((ratio_sams == 2.0000).all(), f\"Sam's expected Volume/Unit ratio == 2.0000 for all rows. Sample:\\n{ratio_sams.value_counts().head()}\")\n",
        "\n",
        "# Test 6.3 — Costco Volume Sales = Unit Sales × 2.0 (Sparkling Ice rows)\n",
        "costco_si = _subset_cleaned(cleaned, \"Costco\", \"Sparkling Ice\")\n",
        "calc_costco_volume = costco_si[\"Unit Sales\"].astype(float) * 2.0\n",
        "max_abs_diff = float((costco_si[\"Volume Sales\"].astype(float) - calc_costco_volume).abs().max())\n",
        "assert_true(max_abs_diff < 1e-6, f\"Costco Volume Sales should equal Unit Sales × 2.0. max_abs_diff={max_abs_diff}\")\n",
        "\n",
        "# Join to final wide df to validate Volume_Sales_SI matches cleaned long volume per Date\n",
        "costco_final = _subset_final(df, \"Costco\")[[\"Date\", \"Volume_Sales_SI\"]].copy()\n",
        "merged = costco_final.merge(costco_si[[\"Date\", \"Volume Sales\", \"Unit Sales\"]], on=\"Date\", how=\"left\")\n",
        "assert_true(merged[\"Volume Sales\"].notna().all(), \"Costco final rows did not match cleaned long rows by Date\")\n",
        "\n",
        "max_abs_diff = float((merged[\"Volume_Sales_SI\"].astype(float) - merged[\"Volume Sales\"].astype(float)).abs().max())\n",
        "assert_true(max_abs_diff < 1e-6, f\"Costco Volume_Sales_SI should match cleaned long Volume Sales. max_abs_diff={max_abs_diff}\")\n",
        "\n",
        "# Expected range\n",
        "assert_all_between(costco_final[\"Volume_Sales_SI\"].astype(float), 68090, 513518, \"Costco Volume_Sales_SI range\")\n",
        "assert_true((costco_final[\"Volume_Sales_SI\"].astype(float) > 0).all(), \"Costco Volume_Sales_SI contains non-positive values\")\n",
        "\n",
        "print(\"✓ Test Group 6 passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 7 — Availability Masks\n",
        "\n",
        "**What this cell does**\n",
        "- Validates `has_promo` and `has_competitor` flags per retailer.\n",
        "- Validates Costco’s competitor price logs are hard-masked (`Log_Price_PL = 0.0`).\n",
        "\n",
        "**Why we do it**\n",
        "- These masks are how the modeling layer handles retailer-specific missing features.\n",
        "\n",
        "**What to look for**\n",
        "- BJ’s/Sam’s: `has_promo = 1`, `has_competitor = 1`.\n",
        "- Costco: `has_promo = 1`, `has_competitor = 0`, and `Log_Price_PL = 0.0` (not NaN).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final = df.copy()\n",
        "\n",
        "# Test 7.1 — BJ's masks\n",
        "bjs_final = _subset_final(df_final, \"BJ's\")\n",
        "assert_true((bjs_final[\"has_promo\"] == 1).all(), \"BJ's expected has_promo=1 for all rows\")\n",
        "assert_true((bjs_final[\"has_competitor\"] == 1).all(), \"BJ's expected has_competitor=1 for all rows\")\n",
        "\n",
        "# Test 7.2 — Sam's masks\n",
        "sams_final = _subset_final(df_final, \"Sam's Club\")\n",
        "assert_true((sams_final[\"has_promo\"] == 1).all(), \"Sam's expected has_promo=1 for all rows\")\n",
        "assert_true((sams_final[\"has_competitor\"] == 1).all(), \"Sam's expected has_competitor=1 for all rows\")\n",
        "\n",
        "# Test 7.3 — Costco masks\n",
        "costco_final = _subset_final(df_final, \"Costco\")\n",
        "assert_true((costco_final[\"has_promo\"] == 1).all(), \"Costco expected has_promo=1 for all rows\")\n",
        "assert_true((costco_final[\"has_competitor\"] == 0).all(), \"Costco expected has_competitor=0 for all rows\")\n",
        "\n",
        "# Test 7.4 — Costco Log_Price_PL = 0.0 (not NaN)\n",
        "assert_true(costco_final[\"Log_Price_PL\"].notna().all(), \"Costco Log_Price_PL should not be NaN\")\n",
        "assert_true((costco_final[\"Log_Price_PL\"].astype(float) == 0.0).all(), \"Costco Log_Price_PL should be exactly 0.0 for all rows\")\n",
        "\n",
        "# Test 7.5 — BJ's/Sam's Log_Price_PL > 0\n",
        "assert_true((bjs_final[\"Log_Price_PL\"].astype(float) > 0).all(), \"BJ's expected Log_Price_PL > 0\")\n",
        "assert_true((sams_final[\"Log_Price_PL\"].astype(float) > 0).all(), \"Sam's expected Log_Price_PL > 0\")\n",
        "\n",
        "print(\"✓ Test Group 7 passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 8 — Log Transformations\n",
        "\n",
        "**What this cell does**\n",
        "- Validates log columns are computed correctly and contain no NaN/inf:\n",
        "  - `Log_Volume_Sales_SI = ln(Volume_Sales_SI)`\n",
        "  - `Log_Base_Price_SI = ln(Base_Price_SI)`\n",
        "  - `Log_Price_PL` behavior differs by retailer (Costco masked to 0.0)\n",
        "\n",
        "**Why we do it**\n",
        "- Log transforms are what the Bayesian model actually consumes; bad logs can silently poison inference.\n",
        "\n",
        "**What to look for**\n",
        "- All log columns are finite.\n",
        "- Ranges roughly match expectations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final = df.copy()\n",
        "\n",
        "# Test 8.1 — Log Volume Sales\n",
        "calc_log_vol = np.log(df_final[\"Volume_Sales_SI\"].astype(float))\n",
        "max_abs_diff = float((df_final[\"Log_Volume_Sales_SI\"].astype(float) - calc_log_vol).abs().max())\n",
        "assert_true(max_abs_diff < 1e-9, f\"Log_Volume_Sales_SI mismatch vs ln(Volume_Sales_SI). max_abs_diff={max_abs_diff}\")\n",
        "assert_true(np.isfinite(df_final[\"Log_Volume_Sales_SI\"].astype(float)).all(), \"Log_Volume_Sales_SI contains non-finite values\")\n",
        "assert_all_between(df_final[\"Log_Volume_Sales_SI\"].astype(float), 9.5, 13.5, \"Log_Volume_Sales_SI range\")\n",
        "\n",
        "# Test 8.2 — Log Base Price\n",
        "calc_log_base = np.log(df_final[\"Base_Price_SI\"].astype(float))\n",
        "max_abs_diff = float((df_final[\"Log_Base_Price_SI\"].astype(float) - calc_log_base).abs().max())\n",
        "assert_true(max_abs_diff < 1e-9, f\"Log_Base_Price_SI mismatch vs ln(Base_Price_SI). max_abs_diff={max_abs_diff}\")\n",
        "assert_true(np.isfinite(df_final[\"Log_Base_Price_SI\"].astype(float)).all(), \"Log_Base_Price_SI contains non-finite values\")\n",
        "assert_all_between(df_final[\"Log_Base_Price_SI\"].astype(float), 2.70, 3.00, \"Log_Base_Price_SI range\")\n",
        "\n",
        "# Test 8.3 — Log Price PL\n",
        "bjs_final = _subset_final(df_final, \"BJ's\")\n",
        "sams_final = _subset_final(df_final, \"Sam's Club\")\n",
        "costco_final = _subset_final(df_final, \"Costco\")\n",
        "\n",
        "calc_bjs_pl = np.log(bjs_final[\"Price_PL\"].astype(float))\n",
        "calc_sams_pl = np.log(sams_final[\"Price_PL\"].astype(float))\n",
        "\n",
        "max_abs_diff = float((bjs_final[\"Log_Price_PL\"].astype(float) - calc_bjs_pl).abs().max())\n",
        "assert_true(max_abs_diff < 1e-9, f\"BJ's Log_Price_PL mismatch. max_abs_diff={max_abs_diff}\")\n",
        "\n",
        "max_abs_diff = float((sams_final[\"Log_Price_PL\"].astype(float) - calc_sams_pl).abs().max())\n",
        "assert_true(max_abs_diff < 1e-9, f\"Sam's Log_Price_PL mismatch. max_abs_diff={max_abs_diff}\")\n",
        "\n",
        "assert_true(costco_final[\"Log_Price_PL\"].notna().all(), \"Costco Log_Price_PL should not be NaN\")\n",
        "assert_true((costco_final[\"Log_Price_PL\"].astype(float) == 0.0).all(), \"Costco Log_Price_PL should be 0.0\")\n",
        "\n",
        "# No NaNs anywhere\n",
        "for c in [\"Log_Volume_Sales_SI\", \"Log_Base_Price_SI\", \"Log_Price_PL\"]:\n",
        "    assert_true(df_final[c].isna().sum() == 0, f\"Found NaN values in {c}\")\n",
        "\n",
        "print(\"✓ Test Group 8 passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 9 — Seasonality and Time Features\n",
        "\n",
        "**What this cell does**\n",
        "- Validates seasonality dummies (`Spring`, `Summer`, `Fall`) are consistent with month mapping and mutually exclusive.\n",
        "- Validates `Week_Number` is monotonic by date within each retailer.\n",
        "- Validates `Week_Number` is globally consistent across retailers for the same calendar date.\n",
        "\n",
        "**Why we do it**\n",
        "- Seasonality/time features are key controls; misalignment causes biased elasticity.\n",
        "\n",
        "**What to look for**\n",
        "- Winter months (Dec/Jan/Feb) should have Spring/Summer/Fall all 0.\n",
        "- Within each retailer, Week_Number should never decrease as Date increases.\n",
        "- For a given Date, all retailers should share the same Week_Number.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final = df.copy()\n",
        "\n",
        "# Test 9.1 — seasonal dummies mutually exclusive, and match month mapping\n",
        "for col in [\"Spring\", \"Summer\", \"Fall\"]:\n",
        "    assert_true(col in df_final.columns, f\"Missing expected seasonal dummy column: {col}\")\n",
        "\n",
        "season_sum = df_final[[\"Spring\", \"Summer\", \"Fall\"]].sum(axis=1)\n",
        "assert_true((season_sum <= 1).all(), \"Seasonal dummies should be mutually exclusive (sum <= 1)\")\n",
        "\n",
        "months = df_final[\"Date\"].dt.month\n",
        "is_spring = months.isin([3, 4, 5])\n",
        "is_summer = months.isin([6, 7, 8])\n",
        "is_fall = months.isin([9, 10, 11])\n",
        "is_winter = months.isin([12, 1, 2])\n",
        "\n",
        "assert_true((df_final.loc[is_spring, \"Spring\"] == 1).all(), \"Spring dummy should be 1 for months 3-5\")\n",
        "assert_true((df_final.loc[~is_spring, \"Spring\"] == 0).all(), \"Spring dummy should be 0 outside months 3-5\")\n",
        "\n",
        "assert_true((df_final.loc[is_summer, \"Summer\"] == 1).all(), \"Summer dummy should be 1 for months 6-8\")\n",
        "assert_true((df_final.loc[~is_summer, \"Summer\"] == 0).all(), \"Summer dummy should be 0 outside months 6-8\")\n",
        "\n",
        "assert_true((df_final.loc[is_fall, \"Fall\"] == 1).all(), \"Fall dummy should be 1 for months 9-11\")\n",
        "assert_true((df_final.loc[~is_fall, \"Fall\"] == 0).all(), \"Fall dummy should be 0 outside months 9-11\")\n",
        "\n",
        "# Winter months: all three 0\n",
        "assert_true((df_final.loc[is_winter, [\"Spring\", \"Summer\", \"Fall\"]].sum(axis=1) == 0).all(), \"Winter months should have Spring/Summer/Fall all 0\")\n",
        "\n",
        "# Test 9.2 — Week_Number monotonic per retailer\n",
        "for retailer in df_final[\"Retailer\"].unique().tolist():\n",
        "    r = df_final[df_final[\"Retailer\"] == retailer].sort_values(\"Date\")\n",
        "    diffs = r[\"Week_Number\"].diff().fillna(0)\n",
        "    assert_true((diffs >= 0).all(), f\"{retailer}: Week_Number decreased for some dates\")\n",
        "\n",
        "# Week_Number = 0 for earliest date in combined dataset\n",
        "min_date = df_final[\"Date\"].min()\n",
        "week0 = df_final.loc[df_final[\"Date\"] == min_date, \"Week_Number\"].unique().tolist()\n",
        "assert_true(len(week0) >= 1 and all(int(w) == 0 for w in week0), f\"Expected Week_Number=0 for earliest date {min_date}, got {week0}\")\n",
        "\n",
        "# Test 9.3 — Week_Number globally consistent per Date\n",
        "wk_counts = df_final.groupby(\"Date\")[\"Week_Number\"].nunique()\n",
        "assert_true(int((wk_counts > 1).sum()) == 0, f\"Week_Number differs across retailers for some dates. Sample:\\n{wk_counts[wk_counts>1].head()}\")\n",
        "\n",
        "print(\"✓ Test Group 9 passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 10 — Final Output Schema\n",
        "\n",
        "**What this cell does**\n",
        "- Validates the final model-ready dataframe has required columns, expected row counts, and no duplicates.\n",
        "- Checks critical columns have no NaNs.\n",
        "- Performs basic dtype sanity checks.\n",
        "\n",
        "**Why we do it**\n",
        "- Ensures the modeling stage receives a stable, contract-compliant dataset.\n",
        "\n",
        "**What to look for**\n",
        "- Required columns exist.\n",
        "- Approximate row counts: BJ’s ~159, Sam’s ~159, Costco ~161 (total ~479).\n",
        "- No duplicate (`Date`, `Retailer`) pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final = df.copy()\n",
        "\n",
        "# Test 10.1 — required columns present\n",
        "required_cols = [\n",
        "    \"Date\", \"Retailer\",\n",
        "    \"Log_Volume_Sales_SI\", \"Log_Base_Price_SI\", \"Promo_Depth_SI\",\n",
        "    \"Log_Price_PL\", \"Log_Price_SI\",\n",
        "    \"Price_SI\", \"Base_Price_SI\", \"Volume_Sales_SI\",\n",
        "    \"Spring\", \"Summer\", \"Fall\",\n",
        "    \"Week_Number\",\n",
        "    \"has_promo\", \"has_competitor\",\n",
        "    \"Promo_Intensity_SI\",\n",
        "]\n",
        "missing = [c for c in required_cols if c not in df_final.columns]\n",
        "assert_true(len(missing) == 0, f\"Missing required columns: {missing}\")\n",
        "\n",
        "# Test 10.2 — row counts per retailer\n",
        "counts = df_final[\"Retailer\"].value_counts().to_dict()\n",
        "\n",
        "bjs_n = int(counts.get(\"BJ's\", 0))\n",
        "sams_n = int(counts.get(\"Sam's Club\", 0))\n",
        "costco_n = int(counts.get(\"Costco\", 0))\n",
        "\n",
        "assert_true(abs(bjs_n - 159) <= 2, f\"BJ's row count expected ~159, got {bjs_n}\")\n",
        "assert_true(abs(sams_n - 159) <= 2, f\"Sam's row count expected ~159, got {sams_n}\")\n",
        "assert_true(abs(costco_n - 161) <= 2, f\"Costco row count expected ~161, got {costco_n}\")\n",
        "assert_true(abs(len(df_final) - 479) <= 5, f\"Total row count expected ~479, got {len(df_final)}\")\n",
        "\n",
        "# Test 10.3 — no duplicate Date-Retailer\n",
        "n_dupes = int(df_final.duplicated(subset=[\"Date\", \"Retailer\"]).sum())\n",
        "assert_eq(n_dupes, 0, f\"Expected 0 duplicate Date-Retailer rows, got {n_dupes}\")\n",
        "\n",
        "# Test 10.4 — no NaN in critical columns\n",
        "critical = [\"Log_Volume_Sales_SI\", \"Log_Base_Price_SI\", \"Promo_Depth_SI\", \"Log_Price_PL\", \"has_promo\", \"has_competitor\"]\n",
        "for c in critical:\n",
        "    n = int(df_final[c].isna().sum())\n",
        "    assert_true(n == 0, f\"Expected 0 NaN in {c}, got {n}\")\n",
        "\n",
        "# Test 10.5 — data types\n",
        "assert_true(np.issubdtype(df_final[\"Date\"].dtype, np.datetime64), f\"Date dtype should be datetime64, got {df_final['Date'].dtype}\")\n",
        "assert_true(df_final[\"Retailer\"].dtype == object or str(df_final[\"Retailer\"].dtype).startswith(\"string\"), f\"Retailer dtype should be object/string, got {df_final['Retailer'].dtype}\")\n",
        "\n",
        "numeric_cols = [c for c in required_cols if c not in [\"Date\", \"Retailer\"]]\n",
        "non_numeric = [c for c in numeric_cols if not (np.issubdtype(df_final[c].dtype, np.number))]\n",
        "assert_true(len(non_numeric) == 0, f\"Expected numeric dtypes for columns: {non_numeric}\")\n",
        "\n",
        "print(\"✓ Test Group 10 passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 11 — Cross-Retailer Consistency\n",
        "\n",
        "**What this cell does**\n",
        "- Re-validates the promo depth formula consistency across retailers.\n",
        "- Ensures adding Costco to the pipeline does **not** change BJ’s or Sam’s outputs.\n",
        "- Checks that `Base_Price_SI >= Price_SI` holds in the vast majority of weeks.\n",
        "\n",
        "**Why we do it**\n",
        "- Costco integration must be additive; it should never perturb the Circana-based retailers.\n",
        "\n",
        "**What to look for**\n",
        "- BJ’s/Sam’s results are identical with and without Costco.\n",
        "- `Base_Price_SI >= Price_SI` in >90% of rows per retailer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pandas.testing import assert_frame_equal\n",
        "\n",
        "# Test 11.1 — Promo_Depth formula same for all (tolerance 0.001)\n",
        "df_final = df.copy()\n",
        "calc = ((df_final[\"Price_SI\"].astype(float) / df_final[\"Base_Price_SI\"].astype(float)) - 1.0).clip(-0.80, 0.50)\n",
        "max_abs_diff = float((df_final[\"Promo_Depth_SI\"].astype(float) - calc).abs().max())\n",
        "assert_true(max_abs_diff <= 0.001, f\"Promo_Depth formula consistency check failed. max_abs_diff={max_abs_diff}\")\n",
        "\n",
        "# Test 11.2 — BJ's/Sam's unchanged when adding Costco\n",
        "cfg_test = PrepConfig(\n",
        "    retailer_filter=\"All\",\n",
        "    include_seasonality=True,\n",
        "    include_promotions=True,\n",
        "    include_time_trend=True,\n",
        "    separate_base_promo=True,\n",
        "    base_price_proxy_window=8,\n",
        "    base_price_imputed_warn_threshold=0.30,\n",
        "    volume_sales_factor_by_retailer={\"Costco\": 2.0},\n",
        "    retailers={\n",
        "        \"BJs\": {\"has_promo\": True, \"has_competitor\": True},\n",
        "        \"Sams\": {\"has_promo\": True, \"has_competitor\": True},\n",
        "        \"Costco\": {\"has_promo\": True, \"has_competitor\": False},\n",
        "    },\n",
        "    retailer_data_contracts=retailer_data_contracts,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "df_2 = ElasticityDataPrep(cfg_test).transform(bjs_path=BJS_PATH, sams_path=SAMS_PATH, costco_path=None)\n",
        "df_3 = ElasticityDataPrep(cfg_test).transform(bjs_path=BJS_PATH, sams_path=SAMS_PATH, costco_path=COSTCO_PATH)\n",
        "\n",
        "circana_retailers = [\"BJ's\", \"Sam's Club\"]\n",
        "\n",
        "a = df_2[df_2[\"Retailer\"].isin(circana_retailers)].sort_values([\"Retailer\", \"Date\"]).reset_index(drop=True)\n",
        "b = df_3[df_3[\"Retailer\"].isin(circana_retailers)].sort_values([\"Retailer\", \"Date\"]).reset_index(drop=True)\n",
        "\n",
        "# Compare all shared columns\n",
        "common_cols = [c for c in a.columns if c in b.columns]\n",
        "a2 = a[common_cols]\n",
        "b2 = b[common_cols]\n",
        "\n",
        "assert_frame_equal(a2, b2, check_dtype=False, check_like=False)\n",
        "\n",
        "# Test 11.3 — Base_Price_SI >= Price_SI in most weeks (>90%)\n",
        "for retailer in df_final[\"Retailer\"].unique().tolist():\n",
        "    r = df_final[df_final[\"Retailer\"] == retailer]\n",
        "    frac = float((r[\"Base_Price_SI\"].astype(float) >= r[\"Price_SI\"].astype(float)).mean())\n",
        "    assert_true(frac > 0.90, f\"{retailer}: expected Base_Price_SI >= Price_SI in >90% of rows, got {frac:.1%}\")\n",
        "\n",
        "print(\"✓ Test Group 11 passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Group 12 — Costco-Specific Edge Cases\n",
        "\n",
        "**What this cell does**\n",
        "- Validates Costco-specific “known truth” weeks for pricing, base price, promo depth, and volume sales.\n",
        "- Validates the low-NP-units fallback week uses `Average Price per Unit`.\n",
        "- Validates Costco average price is never computed from `Dollar Sales / Unit Sales`.\n",
        "\n",
        "**Why we do it**\n",
        "- Costco’s CRX schema differs materially from Circana; these checks prevent regressions.\n",
        "\n",
        "**What to look for**\n",
        "- Week ending 2023-02-05 matches the expected promo-week values.\n",
        "- Week ending 2023-01-01 behaves like a non-promo week.\n",
        "- Low-NP-units week (2024-02-11) uses fallback base price.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final = df.copy()\n",
        "cleaned = prep.cleaned_data\n",
        "\n",
        "costco_final = _subset_final(df_final, \"Costco\")\n",
        "costco_long = _subset_cleaned(cleaned, \"Costco\", \"Sparkling Ice\")\n",
        "\n",
        "# Join final rows to cleaned long rows so we can compare to raw columns (Dollar Sales, Unit Sales, etc.)\n",
        "joined = costco_final.merge(\n",
        "    costco_long[[\n",
        "        \"Date\",\n",
        "        \"Time\",\n",
        "        \"Dollar Sales\",\n",
        "        \"Unit Sales\",\n",
        "        \"Avg Net Price\",\n",
        "        \"Non Promoted Dollars\",\n",
        "        \"Non Promoted Units\",\n",
        "        \"Average Price per Unit\",\n",
        "        \"Base_Avg_Price\",\n",
        "        \"Avg_Price\",\n",
        "        \"Volume Sales\",\n",
        "    ]],\n",
        "    on=\"Date\",\n",
        "    how=\"left\",\n",
        ")\n",
        "assert_true(joined[\"Unit Sales\"].notna().all(), \"Costco final rows did not match cleaned long Costco rows by Date\")\n",
        "\n",
        "# Test 12.1 — Heavy promo week validation (02-05-2023)\n",
        "r = joined.loc[joined[\"Date\"] == pd.Timestamp(\"2023-02-05\")]\n",
        "assert_true(len(r) == 1, f\"Expected 1 Costco row for 2023-02-05, got {len(r)}\")\n",
        "r = r.iloc[0]\n",
        "\n",
        "assert_approx(float(r[\"Avg Net Price\"]), 12.06, tol=0.10, msg=\"Costco 2023-02-05 Avg Net Price\")\n",
        "assert_approx(float(r[\"Base_Price_SI\"]), 16.03, tol=0.10, msg=\"Costco 2023-02-05 Base_Price_SI\")\n",
        "assert_approx(float(r[\"Promo_Depth_SI\"]), -0.248, tol=0.01, msg=\"Costco 2023-02-05 Promo_Depth_SI\")\n",
        "\n",
        "# Volume Sales check for this week\n",
        "assert_approx(float(r[\"Volume_Sales_SI\"]), 356272, tol=100, msg=\"Costco 2023-02-05 Volume_Sales_SI\")\n",
        "\n",
        "# Test 12.2 — Non-promo week validation (01-01-2023)\n",
        "r = joined.loc[joined[\"Date\"] == pd.Timestamp(\"2023-01-01\")]\n",
        "assert_true(len(r) == 1, f\"Expected 1 Costco row for 2023-01-01, got {len(r)}\")\n",
        "r = r.iloc[0]\n",
        "assert_true(abs(float(r[\"Promo_Depth_SI\"])) < 0.001, \"Costco 2023-01-01 Promo_Depth_SI should be near 0\")\n",
        "assert_approx(float(r[\"Price_SI\"]), float(r[\"Base_Price_SI\"]), tol=0.05, msg=\"Costco 2023-01-01 Price_SI should be ~ Base_Price_SI\")\n",
        "\n",
        "# Test 12.3 — Low NP Units fallback (week of 02-11-2024)\n",
        "r = joined.loc[joined[\"Date\"] == pd.Timestamp(\"2024-02-11\")]\n",
        "assert_true(len(r) == 1, f\"Expected 1 Costco row for 2024-02-11, got {len(r)}\")\n",
        "r = r.iloc[0]\n",
        "assert_eq(int(float(r[\"Non Promoted Units\"])), 487, \"Expected Non Promoted Units = 487 for 2024-02-11\")\n",
        "assert_approx(float(r[\"Average Price per Unit\"]), 15.99, tol=0.10, msg=\"Costco 2024-02-11 Average Price per Unit\")\n",
        "assert_approx(float(r[\"Base_Avg_Price\"]), float(r[\"Average Price per Unit\"]), tol=1e-6, msg=\"Fallback base price should equal Average Price per Unit\")\n",
        "\n",
        "# Test 12.4 — Costco Dollar Sales is NOT used for Avg_Price\n",
        "shelf_price = joined[\"Dollar Sales\"].astype(float) / joined[\"Unit Sales\"].astype(float)\n",
        "net_price = joined[\"Avg Net Price\"].astype(float)\n",
        "\n",
        "# For all Costco rows: Price_SI must not be approx equal to shelf price (within $0.10)\n",
        "diff = (joined[\"Price_SI\"].astype(float) - shelf_price).abs()\n",
        "assert_true((diff > 0.10).all(), f\"Costco Price_SI should not equal Dollar/Unit shelf price within $0.10. Sample diffs:\\n{diff.head(10)}\")\n",
        "\n",
        "# Specifically for promo weeks: difference should be > $1.00\n",
        "promo_mask = joined[\"Promo_Depth_SI\"].astype(float) < -0.01\n",
        "promo_diff = diff[promo_mask]\n",
        "assert_true((promo_diff > 1.00).all(), f\"Costco promo weeks: expected net vs shelf diff > $1.00. Sample:\\n{promo_diff.head(10)}\")\n",
        "\n",
        "print(\"✓ Test Group 12 passed\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
